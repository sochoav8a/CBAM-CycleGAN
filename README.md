# ğŸ”¬ Confocal CycleGAN: Unpaired Image-to-Image Translation for Microscopy

<p align="center">
  <img src="results/comparison_0022.png" alt="Comparison 0022" width="45%"/>
  <img src="results/comparison_0047.png" alt="Comparison 0047" width="45%"/>
</p>


[![Python](https://img.shields.io/badge/Python-3.7%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.7%2B-orange.svg)](https://pytorch.org/)


This project implements a CycleGAN model for unpaired image-to-image translation, specifically tailored for confocal microscopy images. It allows transforming images from one microscopy domain to another without requiring paired training data.

## âœ¨ Features

*   **CycleGAN Architecture**: Employs the core CycleGAN framework for robust unpaired image translation.
*   **Configurable Networks**:
    *   **Generator**: Choose between different ResNet block configurations (e.g., `resnet_9blocks`, `resnet_6blocks`). Support for dropout.
    *   **Discriminator**: PatchGAN discriminator for effective adversarial training.
*   **Attention Mechanisms**: Enhance generator performance with integrated attention modules:
    *   CBAM (Convolutional Block Attention Module)
    *   SelfAttention
*   **Perceptual Loss**: Option to use VGG19 Perceptual Loss for improved image quality and feature matching.
*   **Flexible Loss Functions**:
    *   Adversarial Loss: LSGAN (Least Squares GAN) or Vanilla GAN.
    *   Cycle Consistency Loss.
    *   Identity Loss.
*   **Distributed Training**: Supports Distributed Data Parallel (DDP) for multi-GPU training.
*   **Mixed Precision Training**: AMP (Automatic Mixed Precision) for faster training and reduced memory usage.
*   **Detailed Logging & Visualization**:
    *   Comprehensive loss tracking (for epochs and batches).
    *   Automatic generation of loss plots.
    *   Saves sample images during training.
*   **Easy Configuration**: Centralized configuration management via `src/config.py`.
*   **Checkpointing**: Save and load model checkpoints for resuming training or evaluation.

## ğŸ—ï¸ Arquitectura Visualizada

<p align="center">
  <img src="results/CBAM-CycleGAN Architecture_cropped_page-0001.jpg" alt="CBAM-CycleGAN Architecture" width="80%"/>
</p>

## âš™ï¸ Configuration

All major settings, including paths, model architecture, hyperparameters, and training options, are managed in the `src/config.py` file. Before running the project, please review and customize this file according to your dataset and requirements.

Key configurable aspects:
*   Dataset paths (`DATA_DIR`, `TRAIN_DATA_PATH`, `VAL_DATA_PATH`)
*   Image properties (`IMG_HEIGHT`, `IMG_WIDTH`, `INPUT_CHANNELS`, `OUTPUT_CHANNELS`)
*   Model types (`GEN_TYPE`, `DISC_TYPE`) and parameters (`NGF`, `NDF`, `N_LAYERS_DISC`)
*   Attention mechanisms (`USE_ATTENTION`, `ATTENTION_TYPE`)
*   Loss weights (`LAMBDA_CYCLE`, `LAMBDA_IDENTITY`, `LAMBDA_VGG`)
*   Training parameters (`NUM_EPOCHS`, `BATCH_SIZE`, `LR_G`, `LR_D`)

## ğŸš€ Getting Started

### Prerequisites

*   Python 3.7+
*   PyTorch 1.7+
*   NumPy
*   Matplotlib
*   tqdm


It's recommended to set up a virtual environment:
```bash
python -m venv .venv
source .venv/bin/activate
pip install torch torchvision torchaudio numpy matplotlib tqdm
```

### Data Preparation

1.  Organize your confocal microscopy image data.
2.  Update the `TRAIN_DATA_PATH` and `VAL_DATA_PATH` in `src/config.py` to point to your training and validation `.npz` files (as suggested by `dataset.py`). Ensure your dataset format is compatible with `NpzImageDataset`.
3.  The project expects images to be normalized to the `[-1, 1]` range.

### Training

To start training the CycleGAN model:

```bash
python src/train.py
```

If you are using distributed training with multiple GPUs (e.g., 2 GPUs):
```bash
torchrun --nproc_per_node=2 src/train.py
```

*   Training progress, losses, and sample images will be saved to the `OUTPUT_DIR` specified in `config.py`.
*   Model checkpoints will be saved in the `CHECKPOINT_DIR`.

### Evaluation

To evaluate a trained model and generate translated images:

1.  Ensure your trained generator checkpoints (e.g., `latest_netG_A2B.pth`) are in the `CHECKPOINT_DIR`.
2.  Update `EVAL_CHECKPOINT_G_A2B` (and `EVAL_CHECKPOINT_G_B2A` if needed) in `src/config.py` with the desired checkpoint names.
3.  Prepare your evaluation dataset and update relevant paths if different from training.

Run the evaluation script:
```bash
python src/eval_3.py
```
Generated samples will be saved in `EVAL_SAMPLES_DIR`.

## ğŸ“ Project Structure

\`\`\`
.
â”œâ”€â”€ data/                       # (Create this directory) For your datasets
â”œâ”€â”€ models/                     # Model definitions (generators, discriminators)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ networks.py
â”œâ”€â”€ outputs/                    # (Created automatically) Output directory
â”‚   â”œâ”€â”€ checkpoints/            # Saved model checkpoints
â”‚   â”œâ”€â”€ samples_eval/           # Evaluation image samples
â”‚   â””â”€â”€ samples_train/          # Training image samples
â”œâ”€â”€ resultados/                 # (Existing directory, purpose?)
â”œâ”€â”€ src/                        # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py               # Main configuration file
â”‚   â”œâ”€â”€ dataset.py              # Dataset loading and preprocessing
â”‚   â”œâ”€â”€ evaluate.py             # Evaluation script
â”‚   â”œâ”€â”€ eval.py                 # (Alternative evaluation script)
â”‚   â”œâ”€â”€ eval_2.py               # (Alternative evaluation script)
â”‚   â”œâ”€â”€ eval_3.py               # (Alternative evaluation script)
â”‚   â”œâ”€â”€ plot_losses.py          # Script to plot loss curves
â”‚   â”œâ”€â”€ train.py                # Main training script
â”‚   â”œâ”€â”€ train_r.py              # Train recovery
â”‚   â”œâ”€â”€ utils.py                # Utility functions (schedulers, saving, etc.)
â”‚   â””â”€â”€ vgg_loss.py             # VGG perceptual loss implementation
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ README.md                   # This file
â””â”€â”€ ... (other project files)
\`\`\`

## ğŸ¤ Contributing

Contributions are welcome! If you'd like to contribute, please follow these steps:
1. Fork the repository.
2. Create a new branch (`git checkout -b feature/your-feature-name`).
3. Make your changes.
4. Commit your changes (`git commit -m 'Add some feature'`).
5. Push to the branch (`git push origin feature/your-feature-name`).
6. Open a Pull Request.
